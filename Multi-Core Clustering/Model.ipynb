{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61c6ab81",
   "metadata": {},
   "source": [
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ca691",
   "metadata": {},
   "source": [
    "### Fuzzy C-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc97bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.stats import skew\n",
    "import skfuzzy as fuzz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset into a pandas DataFrame\n",
    "df = pd.read_csv('Final Dataset.csv')\n",
    "\n",
    "\n",
    "# Extract numerical features\n",
    "numerical_data = df.drop(columns=['collection_logical_name' , 'collection_name' , 'user' , 'collections_events_type' , 'machine_id' , \n",
    "                                  'instance_index' , 'alloc_collection_id' , 'collection_type' , 'collection_id' , 'instance_events_type' , 'Unnamed: 0.1'])\n",
    "\n",
    "\n",
    "# Convert to numpy array\n",
    "numerical_array = numerical_data.values\n",
    "\n",
    "\n",
    "# Fuzzy clustering\n",
    "n_clusters = 5  # You can adjust this as needed\n",
    "cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(numerical_array.T, n_clusters, 2, error=0.005, maxiter=1000)\n",
    "\n",
    "# Assign each data point to the cluster with highest membership degree\n",
    "cluster_membership = np.argmax(u, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Scatter plot of clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(numerical_array[:, 0], numerical_array[:, 1], c=cluster_membership, cmap='viridis', s=50, alpha=0.5)\n",
    "plt.title('Fuzzy C-Means Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f5511c",
   "metadata": {},
   "source": [
    "(https://github.com/user-attachments/assets/64615040-166a-42c4-8451-a385fc1361e7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30876d0d",
   "metadata": {},
   "source": [
    "### MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dc5830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load pre-processed dataset into a pandas DataFrame\n",
    "df = pd.read_csv('Final Dataset.csv')\n",
    "\n",
    "# Extract numerical features\n",
    "numerical_data = df.drop(columns=['collection_logical_name', 'collection_name', 'user', 'collections_events_type', 'machine_id',\n",
    "                                  'instance_index', 'alloc_collection_id', 'collection_type', 'collection_id', 'instance_events_type', 'Unnamed: 0.1'])\n",
    "\n",
    "# Handle missing values by imputing with mean values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(numerical_data)\n",
    "\n",
    "# Define a function to fit MiniBatchKMeans with n_jobs=-1\n",
    "def fit_kmeans(X):\n",
    "    kmeans = MiniBatchKMeans(n_clusters=3, batch_size=100, n_init=3, init='k-means++')\n",
    "    kmeans.fit(X)\n",
    "    return kmeans\n",
    "\n",
    "# Train MiniBatchKMeans clustering model with multi-core processing using joblib\n",
    "kmeans = Parallel(n_jobs=-1)(delayed(fit_kmeans)(X_imputed) for _ in range(10))\n",
    "\n",
    "# Use PCA to reduce dimensionality for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_imputed)\n",
    "\n",
    "\n",
    "\n",
    "# Visualize the clustering results without centroid labels\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans[0].labels_, cmap='viridis')\n",
    "# plt.scatter(kmeans[0].cluster_centers_[:, 0], kmeans[0].cluster_centers_[:, 1], marker='x', s=100, c='red')\n",
    "plt.title('MiniBatchKMeans Clustering with Multi-Core Processing')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3a043c",
   "metadata": {},
   "source": [
    "(https://github.com/user-attachments/assets/072c561b-8efe-4701-909d-84d92948914e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4c8094",
   "metadata": {},
   "source": [
    "### OpenMP - numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f9633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from numba import njit, prange\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# Load pre-processed dataset into a pandas DataFrame\n",
    "df = pd.read_csv('Final Dataset.csv')\n",
    "\n",
    "# Extract numerical features\n",
    "numerical_data = df.drop(columns=['collection_logical_name', 'collection_name', 'user', 'collections_events_type', 'machine_id',\n",
    "                                  'instance_index', 'alloc_collection_id', 'collection_type', 'collection_id', 'instance_events_type', 'Unnamed: 0.1'])\n",
    "\n",
    "# Impute missing values in the numerical features\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(numerical_data)\n",
    "\n",
    "# Standardize the imputed numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "\n",
    "# Perform dimensionality reduction using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Define the number of clusters\n",
    "n_clusters = 4\n",
    "\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "@njit(parallel=True)\n",
    "def kmeans_parallel(X, n_clusters, max_iter=300):\n",
    "    n_samples, n_features = X.shape\n",
    "    centers = X[np.random.choice(n_samples, n_clusters, replace=False)]\n",
    "    labels = np.zeros(n_samples, dtype=np.int64)\n",
    "    \n",
    "    for _ in prange(max_iter):\n",
    "        new_centers = np.zeros((n_clusters, n_features))\n",
    "        counts = np.zeros(n_clusters)\n",
    "        \n",
    "        for i in prange(n_samples):\n",
    "            dists = np.zeros(n_clusters)\n",
    "            for j in prange(n_clusters):\n",
    "                dists[j] = euclidean_distance(centers[j], X[i])\n",
    "            label = np.argmin(dists)\n",
    "            labels[i] = label\n",
    "            new_centers[label] += X[i]\n",
    "            counts[label] += 1\n",
    "        \n",
    "        for j in prange(n_clusters):\n",
    "            if counts[j] > 0:\n",
    "                centers[j] = new_centers[j] / counts[j]\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "# Perform KMeans clustering on the data in parallel\n",
    "cluster_labels = kmeans_parallel(X_pca, n_clusters)\n",
    "\n",
    "# Add the cluster labels to the original DataFrame\n",
    "df['cluster_label'] = cluster_labels\n",
    "\n",
    "# Visualize the clustered data\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in range(n_clusters):\n",
    "    plt.scatter(X_pca[cluster_labels == cluster, 0], X_pca[cluster_labels == cluster, 1], label=f'Cluster {cluster}')\n",
    "plt.title('KMeans Clustering with PCA Visualization')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace0d40c",
   "metadata": {},
   "source": [
    "(https://github.com/user-attachments/assets/a0d97f47-a147-4f07-b223-43928745a889)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef60c02",
   "metadata": {},
   "source": [
    "### BIRCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce5f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load pre-processed dataset into a pandas DataFrame\n",
    "df = pd.read_csv('Final Dataset.csv')\n",
    "\n",
    "# Extract numerical features\n",
    "numerical_data = df.drop(columns=['collection_logical_name', 'collection_name', 'user', 'collections_events_type', \n",
    "                                   'machine_id', 'instance_index', 'alloc_collection_id', 'collection_type', \n",
    "                                   'collection_id', 'instance_events_type', 'Unnamed: 0.1'])\n",
    "\n",
    "# Handle missing values by imputing with mean (you can choose a different strategy if needed)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "numerical_data_imputed = imputer.fit_transform(numerical_data)\n",
    "\n",
    "# Standardize the imputed numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_data_scaled = scaler.fit_transform(numerical_data_imputed)\n",
    "\n",
    "# Initialize the BIRCH clustering algorithm\n",
    "birch = Birch(threshold=0.5, branching_factor=50, n_clusters=None)\n",
    "\n",
    "# Fit the BIRCH model on the scaled numerical data\n",
    "birch.fit(numerical_data_scaled)\n",
    "\n",
    "# Get the cluster labels for each data point\n",
    "cluster_labels = birch.labels_\n",
    "\n",
    "# Add the cluster labels to the original DataFrame\n",
    "df['cluster_label'] = cluster_labels\n",
    "\n",
    "# Apply PCA to reduce data to 2 dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(numerical_data_scaled)\n",
    "\n",
    "# Plot the data points colored by cluster labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_data[:, 0], pca_data[:, 1], c=cluster_labels, cmap='viridis')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('BIRCH Clustering Visualization')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da5b447",
   "metadata": {},
   "source": [
    "(https://github.com/user-attachments/assets/2973345f-1f8f-4330-bb06-f61ee258a698)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
